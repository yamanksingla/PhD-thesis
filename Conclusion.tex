%\addcontentsline{toc}{chapter}{Conclusion and an Outlook for Future Work}
\chapter{Conclusion and an Outlook for Future Work}
\label{chapter:conclusion}


This thesis has explored the intersection of communication theory, behavioral science, and artificial intelligence, with a particular focus on explaining, understanding, and optimizing human behavior through large-scale modeling approaches. Our work builds upon the fundamental seven-factor model of communication—communicator, message, channel, time of receipt, receiver, time of behavior, and receiver's behavior—while leveraging unprecedented access to digital behavioral data to advance both explanatory and predictive approaches to behavioral science.
In the domain of persuasion strategy analysis, we have made significant contributions to understanding the mechanisms of influence in advertising. Through comprehensive research spanning marketing, social psychology, and machine learning literature, we developed the most extensive framework of generic persuasion strategies to date. This work was supported by the creation and release of the first datasets for studying persuasion strategies in both image and video advertisements. 


We discover that existing Large Language Models (LLMs), despite their remarkable capabilities in various domains, are inherently limited in modeling behavior due to the systematic removal of behavioral data during training. To address this limitation, we developed the Large Content and Behavior Models (LCBM), which integrates all seven factors of communication to create more comprehensive models of human behavior. To support future research in this area, we released extensive behavior instruction fine-tuning data derived from over 40,000 YouTube videos and 168 million Twitter posts. Additionally, we established new benchmarks for evaluating joint content-behavior understanding, encompassing both predictive and descriptive tasks.


We also made significant strides in demonstrating how behavioral signals can enhance content understanding. Our research showed substantial improvements across 46 different tasks spanning 23 benchmark datasets across language, audio, text, and video modalities. We proposed a scalable approach to enhance Vision Language Models (VLMs) without requiring significant architectural changes, making our improvements readily accessible to the broader research community. These results strongly validate our hypothesis that behavioral responses provide valuable signals for content understanding, opening new avenues for improving AI systems' comprehension capabilities.


In the realm of content generation, we made contributions towards generating performant content in both text and visual domains. Through our work on memorability optimization, we developed Henry, a model achieving a 44\% improvement in memorability scores of the generated content from the starting point. This represents the first successful application of synthetic data in a domain previously lacking large-scale training resources. In the visual domain, we addressed the critical need for engagement-optimized image generation through the development of EngageNet and the creation of EngagingImageNet, a comprehensive dataset of 168 million tweets with associated media and engagement metrics. Our introduction of Engagement Arena, the first automated benchmark for assessing the engagement potential of text-to-image models, provides the research community with a valuable tool for evaluating and improving engagement-oriented image generation techniques.


\section{Limitations and Future Work}
\label{section:conclusion-limitations and future work}

Despite these significant contributions, our research has several important limitations that must be acknowledged. First, our models exhibit fundamental scale-dependent performance limitations, where behavioral specialization cannot overcome the advantages of larger parameter counts in pure content understanding tasks, as evidenced by LCBM's consistent underperformance compared to significantly larger models like GPT-4V. Second, we observe domain specificity constraints in behavioral transfer, with models trained on platform-specific data (e.g., YouTube) showing substantial performance degradation when applied to different domains (e.g., email marketing), suggesting that behavioral patterns are more context-dependent than initially hypothesized. Third, our approaches face scalability challenges with longer and more complex content, particularly evident in the verbalization approach used in LCBM, where performance degrades significantly for content exceeding certain thresholds (videos over 300 seconds or emails with extensive visual content). Fourth, the quality and availability of behavioral signals create dependencies that limit model applicability—performance drops substantially for content with sparse engagement data or when behavioral metrics are influenced by external events and temporal variance. Fifth, our evaluation frameworks, particularly for memorability prediction, are constrained by relatively low human consistency scores (ranging from 0.55 to 0.78), indicating fundamental challenges in behavioral assessment that set theoretical performance ceilings. Finally, our datasets and models exhibit inherent biases stemming from platform demographics, self-selection effects, and algorithmic influences, which may limit generalizability across diverse populations and cultural contexts. These limitations underscore the need for continued research in developing more robust, scalable, and generalizable approaches to behavioral modeling.

Looking ahead, this research opens several promising directions for future work. The integration of behavioral data into AI systems could lead to more nuanced and context-aware models that better understand and predict human responses. Concretely, we visualize the following avenues for automated behavioral sciences in the near future:
\begin{enumerate}
    \item \textbf{Infinite Personalization}: Before the invention of the printing press, each document had to be written with manual effort. Content production was the limiting factor in communication. The invention of the printing press made it possible to mass-produce content. However, delivery was still limited. While newspapers began to be printed, their area of influence was limited to a certain small geographical boundary. Delivery was the limiting factor then. Steam engines helped solve some of that problem. Still, the extent of delivery was limited, and the speed of delivery was slow. It was not until the invention of the internet and mobile devices that the delivery problem was completely solved. Now, anyone can instantly deliver any piece of content to any other person. The next limiting factor in communication is the time and human labor cost of producing content. This limits a communicator to send out the same message to all the receivers. Further, as both ours and several other research studies have shown, humans are bad at predicting the behavior of others; we need techniques to produce performant content. This will enable infinite personalization, a personalized way of communicating between a communicator and a receiver, with the aim of fulfilling the shared goals.

    

    \item \textbf{Simulating Digital Humans and Digital Societies}: At the heart of social simulation lie two perspectives \cite{gilbert2005simulation}: 1) the dynamic feedback or interaction among individuals, and 2) the states of the population, either as a collective whole or as distinct groups. By simulating social activities, researchers and practitioners can predict the future evolution of individuals and groups. In addition, they facilitate experimental environments through interventions. Social simulation can be implemented in two forms: digital humans \cite{park2023generative,chopard1998cellular,Argyle_2023} and digital societies \cite{khandelwal2023large,bhattacharyyasocia2024,si2023long,khurana2023behavior,santurkar2023whose}. In digital human simulation, either human-crafted rules or parameterized models are used to depict the behavior of individuals (referred to as agents) who interact with others, in societal simulation, equations or models are used to model the society as a whole including the societal non-linear interactions.

    The emergence of Large Language Models as behavioral proxies represents a paradigm shift in social science methodology. Recent breakthroughs have demonstrated that LLMs can be conditioned to embody specific demographic and psychological profiles, creating "silicon samples" that mirror human populations \cite{argyle2023out,park2024generative}. This capability extends beyond simple text generation to encompass the replication of complex human attitudes, decision-making patterns, and social behaviors with remarkable fidelity.

    The implications of this technological advancement are profound. LLMs can now be adapted to reflect the worldviews and opinions of populations shaped by specific information environments \cite{chu2023language}, enabling researchers to explore how media consumption patterns influence collective beliefs and behaviors. The integration of persona variables—demographic, social, and behavioral characteristics—into language models \cite{hu2024quantifying} has opened new avenues for understanding individual differences in perception and judgment, particularly in contexts where human opinions naturally diverge.

    These developments converge toward a future where digital twins of human populations could serve as testing grounds for social interventions, policy proposals, and communication strategies. The ability to simulate complex social phenomena such as information cascades, opinion polarization, and collective decision-making \cite{wang2025user} promises to revolutionize how we study and predict human behavior at scale. Yet this power comes with the responsibility to ensure these simulations authentically represent the diversity of human experience, as early investigations reveal significant gaps between model outputs and the true breadth of human opinion \cite{santurkar2023whose,rescala2024can}.

    The key to building these simulation models lies in leveraging the vast digital footprint left by these observable factors. Both physical and digital interactions contain these signals. For instance, consider a physical political banner displayed by the political campaign of Kamala Harris saying ``For The People'' in a busy city such as San Francisco and viewed by office-goers, receiving various reactions such as hopeful comments, visible disdain, or cold indifference. Analogously in the digital domain, a tweet by a figure like Donald Trump saying ``Make America Great Again'' receives likes, retweets, and comments, whether positive or negative. However, digital signals are far more accessible and recorded in structured datasets, making them ideal for training a Foundation Model. Digital Analytics have been recording such digital signals for decades. Digital analytics involves collecting, analyzing, and interpreting data from digital platforms to capture user behavior. This data typically includes messages sent by a marketer in the form of websites, apps, or digital products and records actions such as clicks, page views, session durations, and navigation patterns, which provide insights into user behavior over a period of time. We have made some initial strides towards achieving this in our recent work \cite{bhattacharyyasocia2024}.


    \item \textbf{Measuring persuasiveness and engagement potential of automated agents}: Large Language Models (LLMs) have demonstrated proficiency in content generation and, more recently, in human persuasion through the production of persuasive content \cite{durmus2024persuasion,singh2024measuring}. The development of such systems that are capable of generating verifiably persuasive messages presents both opportunities and challenges for society. On one hand, such systems could positively impact domains like advertising and social good, such as addressing vaccine hesitancy \cite{sekar2021domestic,PRWeek_DefeatDespairCOVID19}. Conversely, these systems could have detrimental effects if used to influence political inclinations \cite{tappin2023quantifying}, propagate misinformation \cite{lukito2020coordinating}, or manipulate consumer choices \cite{boerman2017online}.

    The sophistication of LLMs in recognizing and generating persuasive content has reached remarkable levels, with models now capable of distinguishing argument quality and predicting individual responses with human-level accuracy \cite{rescala2024can}. This capability raises fundamental questions about the nature of persuasion itself and challenges our understanding of what makes arguments compelling across different populations. The emergence of ensemble approaches that can surpass individual human performance in persuasion detection suggests we are approaching a threshold where artificial systems may possess more nuanced understanding of persuasive mechanisms than any single human observer. This convergence of artificial and human judgment capabilities \cite{santurkar2023whose} underscores the critical importance of developing evaluation frameworks that capture the full spectrum of human diversity in persuasive response.

    Given these potential societal impacts, it is crucial to develop rigorous methods for studying, measuring, benchmarking, and monitoring the persuasive capabilities of AI models. We have made some initial strides towards achieving this in our recent works \cite{singh2024measuring,khurana2023behavior}.


    \item \textbf{Automatically explaining human behavior}: While the behavioral science communities are divided into prediction and explanation, and the communities are growing farther apart, the fundamental curiosity of humans is to learn more about themselves and their environment and how it operates. While predictions may be increasingly more and more accurate, if the mechanism is not well understood, the fundamental human curiosity is not satisfied. As a community, our ongoing commitment is to uncover the mechanisms underlying human behavior. However, we have to discover methods that carry both higher predictive power and are scalable. This may be solved in the future by using advanced tools such as simulations and data from natural experiments to bridge the gap between prediction and explanation.


         \item \textbf{Rethinking free will in the age of behavioral modeling}: A central and unresolved question in behavioral science is how we should define and understand free will in the context of human decision-making. One practical definition is that free will is the lack of predictability in human actions—if a model is able to predict an individual's next action to a high degree of accuracy, then the existence of true free will comes into question. Traditional views often assume that individuals possess agency and autonomy in their choices. However, the development of models such as LCBM \cite{khandelwal2023large} and advances in persuasion modeling \cite{singh2024measuring,khurana2023behavior} challenge this notion by demonstrating that human behavior can be systematically predicted, influenced, and even optimized using large-scale data and machine learning. This perspective aligns with a growing body of evidence from neuroscience and psychology, which suggests that many decisions are shaped by unconscious processes, prior experiences, and environmental cues—sometimes before conscious awareness is even possible (see, e.g., Libet's experiments \cite{libet1985unconscious,libet1993time}, studies on priming \cite{bargh1996automaticity}, and behavioral economics research on automaticity \cite{tversky1985framing,ariely2003coherent,johnson2003defaults}). Such findings lend support to anti-free will arguments, raising profound questions about the true nature of autonomy.


    Our work in this thesis explores these issues from a modeling and machine learning perspective, showing how behavioral data and computational models can capture and even influence human actions at scale. Ultimately, the next frontier for science will be to rigorously investigate the degree to which human actions are governed by free will versus being determined by past experiences, environmental context, and external influences. Addressing this will require interdisciplinary collaboration, combining empirical research, philosophical inquiry, and advances in AI to better understand the boundaries and mechanisms of human agency.



%    \item \textbf{Increasing use of natural experiments}: Most behavioral science still relies on causal study designs and experimentation. However, causal studies are limited because of the amount of data \cite{dunning2012natural,tan2014effect,singh2024measuring}
\end{enumerate}


\section{Societal and Ethical Implications of Behavior Optimized AI}

The development of AI systems capable of understanding, predicting, and optimizing human behavior represents a significant technological advancement with far-reaching societal implications. Our work throughout this thesis—spanning persuasion strategy analysis in advertising, behavioral signal integration for content understanding, content generation optimized for memorability and engagement, and comprehensive behavioral modeling—demonstrates both the tremendous potential and the inherent risks of behavior-optimized AI. This section synthesizes the ethical considerations that emerge from our research and similar work in the field, providing a comprehensive framework for understanding the broader implications of this technology.

\subsection{Dual-Use Nature and Manipulation Risks}

Behavior-optimized AI systems exhibit a fundamental dual-use characteristic: the same technologies that can benefit society can also be misused for harmful purposes \cite{singh2024measuring,khurana2023behavior}. Our research demonstrates that AI models can be trained to generate content optimized for specific behavioral outcomes—whether increasing memorability, engagement, or persuasive impact. While such capabilities offer tremendous potential for positive applications in education, public health campaigns, and user-centered design, they simultaneously create unprecedented opportunities for manipulation and deception.

The manipulation potential of these systems manifests in several critical domains. \textbf{Political manipulation} represents perhaps the most concerning risk, where behavior-optimized AI could be deployed to shape political inclinations through carefully crafted, engagement-maximized content that exploits psychological vulnerabilities rather than presenting balanced information \cite{tappin2023quantifying}. \textbf{Misinformation amplification} poses another significant threat, as these systems could be used to create and disseminate false information in formats optimized for viral spread and cognitive acceptance \cite{lukito2020coordinating}. Additionally, \textbf{consumer manipulation} risks emerge when such technologies are used to encourage ill-informed purchasing decisions or exploit behavioral biases for commercial gain \cite{boerman2017online}.

Our work has consistently acknowledged these risks and implemented safeguards including Acceptable Use Policies that explicitly prohibit deployment in high-risk domains such as political campaigning, spam generation, and deceptive advertising. However, the fundamental challenge remains: how can we quantitatively measure the "manipulation potential" of a behavioral model and develop robust safeguards that prevent misuse while preserving beneficial capabilities?

\subsection{Privacy and Data Protection Challenges}

The development of behavior-optimized AI systems necessitates the collection and analysis of vast amounts of behavioral data, raising significant privacy concerns. Throughout our research—from analyzing persuasion strategies in advertising datasets to integrating behavioral signals like eye movements, comments, and engagement metrics—we have implemented comprehensive privacy-preserving strategies, including systematic removal of personally identifiable information (PII), restriction of data collection to enterprise accounts identified through structured knowledge bases, and aggregation of behavioral signals to prevent individual re-identification.

However, the scale and granularity of behavioral data required for effective modeling present ongoing challenges. Even aggregated behavioral data can potentially reveal sensitive information about individuals or groups, particularly when combined with other data sources. The development of formal differential privacy mechanisms \cite{dwork2014algorithmic} and federated learning approaches \cite{mcmahan2017communication} represents promising directions for addressing these concerns, but significant technical challenges remain in balancing privacy protection with model utility in high-dimensional, multimodal behavioral modeling scenarios.

\subsection{Bias, Fairness, and Representation}

Behavioral data inherently reflects the biases present in the platforms and populations from which it is collected. Our research has consistently acknowledged that data sourced from diverse platforms—including advertising datasets for persuasion analysis, social media platforms for engagement modeling, eye-tracking studies for attention modeling, and video platforms for behavioral understanding—may exhibit demographic skews, self-selection bias, and algorithmic influences that could lead to uneven model performance across different user groups or reinforce existing societal biases.

These biases manifest in multiple dimensions: \textbf{demographic biases} related to age, gender, and socioeconomic background may influence how persuasion strategies are interpreted and modeled; \textbf{cultural biases} may affect the recognition and effectiveness of behavioral interventions across different cultural contexts; and \textbf{temporal biases} may result from the dynamic nature of online platforms and changing user behaviors over time.

Addressing these challenges requires comprehensive bias mitigation strategies including adversarial de-biasing techniques \cite{zhang2018mitigating}, fairness-aware learning algorithms \cite{hardt2016equality}, and systematic bias auditing using subgroup performance analysis and counterfactual fairness testing. However, the fundamental challenge of ensuring fair and representative behavioral modeling across diverse global populations remains an active area of research.

\subsection{Autonomy, Consent, and Human Agency}

The development of increasingly sophisticated behavior prediction and optimization systems raises profound questions about human autonomy and agency. If AI systems can accurately predict and influence human behavior, what does this mean for individual freedom of choice and decision-making? Our work touches on these philosophical questions, particularly in the context of free will and determinism, suggesting that human behavior may be more predictable and influenceable than commonly assumed.

The concept of informed consent becomes particularly complex in the context of behavior-optimized AI. While users may consent to data collection for service improvement, they may not fully understand how their behavioral data will be used to create models capable of predicting and influencing their future actions. This asymmetry of understanding and power raises questions about the validity of consent in these contexts and the need for more transparent and comprehensible disclosure practices.

\subsection{Accountability and Governance Frameworks}

The deployment of behavior-optimized AI systems requires robust governance frameworks that can address the complex ethical challenges while enabling beneficial applications. Our research has contributed to this discussion through various approaches: implementing acceptable use policies for persuasion analysis datasets, developing controlled evaluation frameworks for engagement optimization, establishing privacy-preserving protocols for behavioral data collection, and proposing "Constitutional AI" approaches for persuasion, where models are trained to adhere to auditable ethical principles developed in collaboration with ethicists, psychologists, and policymakers.

However, significant challenges remain in developing effective governance mechanisms. \textbf{Technical auditing} requires new methods for assessing model behavior, detecting potential misuse, and ensuring compliance with ethical guidelines. \textbf{Regulatory frameworks} must balance innovation with protection, requiring close collaboration between technologists, policymakers, and civil society organizations. \textbf{Industry standards} need to be developed to ensure responsible development and deployment practices across the technology sector.

\subsection{Long-term Societal Implications}

The widespread deployment of behavior-optimized AI systems could fundamentally alter the nature of human communication and social interaction. The possibility of "infinite personalization"—where every message is optimized for maximum effectiveness with specific individuals—could create unprecedented levels of persuasive power while potentially undermining the shared information environment necessary for democratic discourse.

The development of digital societies and human simulation capabilities raises additional concerns about the potential for creating echo chambers, amplifying existing divisions, or manipulating public opinion at scale. While these technologies offer tremendous potential for understanding and addressing social challenges, they also create new risks that must be carefully managed.

\subsection{Toward Responsible Development}

Addressing these challenges requires a multi-faceted approach combining technical innovation with ethical reflection and regulatory oversight. Key principles for responsible development include:

\textbf{Transparency and Explainability}: Developing models whose decision-making processes can be understood and audited, enabling meaningful oversight and accountability.

\textbf{Participatory Design}: Involving diverse stakeholders, including affected communities, in the design and deployment of behavior-optimized AI systems.

\textbf{Continuous Monitoring}: Implementing systems for ongoing assessment of model behavior, societal impact, and potential misuse.

\textbf{Adaptive Governance}: Creating flexible regulatory frameworks that can evolve with technological developments while maintaining core ethical principles.

The societal implications of behavior-optimized AI are profound and multifaceted. While our research demonstrates the tremendous potential of these technologies for beneficial applications, it also highlights the critical importance of addressing ethical challenges proactively. The future development of this field must prioritize not only technical advancement but also the careful consideration of societal impact, ensuring that the power to understand and influence human behavior is wielded responsibly and in service of human flourishing.


\section{Open Research Questions}

To spur progress, we outline key open research questions, categorized into four thematic areas.

\subsection*{Core Modeling and Architectural Challenges}
\begin{itemize}
    \item \textbf{Architectural Innovation for Multimodal Behavior:} What architectural innovations are needed to effectively fuse textual, visual, and behavioral modalities in a single, coherent model? A central challenge is balancing the computational overhead of incorporating diverse behavioral signals against the performance gains in downstream tasks.
    
    \item \textbf{Modeling Long-term and Sequential Behavioral Dynamics:} Current models often predict immediate, atomic actions (e.g., clicks, likes). A more profound understanding requires modeling long-term, sequential dynamics. How does a series of information exposures shape a user's beliefs or habits over months or years? This necessitates a shift from stateless to stateful user models that capture evolving internal states (e.g., knowledge, preferences, attitudes).

    \item \textbf{Data-Efficient Behavioral Modeling:} How can we develop accurate behavioral models in domains with scarce data? This involves exploring few-shot, zero-shot, and transfer learning to leverage knowledge from data-rich environments (e.g., social media) for data-poor contexts (e.g., public health campaigns, niche product markets, or B2B interactions).
\end{itemize}

\subsection*{Causality, Interpretability, and Intervention}
\begin{itemize}
    \item \textbf{Integrating Causality into Predictive Models:} A key frontier is the integration of causal inference with predictive modeling. Future work should focus on hybrid models that combine the predictive power of large-scale machine learning with the explanatory power of causal reasoning. This could involve techniques for automatically identifying and leveraging natural experiments within massive datasets or developing architectures that learn causal graphs from observational and interventional data, moving from 'what' users will do to 'why' they do it.
    
    \item \textbf{From Prediction to Automated Intervention:} How can we build systems that move beyond passive prediction to actively recommend or perform interventions that achieve specific behavioral outcomes? This requires developing frameworks that can optimize for long-term goals (e.g., user well-being, sustained engagement, public health adherence) in complex, dynamic environments, while navigating the ethical considerations of such automated influence.

    \item \textbf{Generating Persuasive and Trustworthy Explanations:} Beyond predicting behavior, can we generate faithful explanations for *why* people act as they do? Furthermore, can we design models whose own recommendations are accompanied by explanations that are not only interpretable but also persuasive and trust-inspiring? How can we measure the behavioral impact of such machine-generated explanations?
\end{itemize}

\subsection*{Ethical and Societal Considerations}
\begin{itemize}
    \item \textbf{Ethical Frameworks for Persuasive Technologies:} As AI systems become more persuasive, it is imperative to develop robust ethical frameworks to guide their deployment. An actionable research direction is the creation of 'Constitutional AI' for persuasion, where models are trained to adhere to auditable ethical principles (e.g., transparency, fairness, respect for autonomy), co-developed with ethicists, psychologists, and policymakers.

    \item \textbf{Quantifying and Mitigating Manipulation Risk:} How can we quantitatively measure the 'manipulation potential' of a behavioral model? Can we develop adversarial training techniques, formal verification methods, or auditing protocols to build models that are robust against misuse for malicious influence while preserving their beneficial capabilities?

    \item \textbf{Building and Validating Digital Societies:} How can we construct and validate large-scale, multi-agent simulations of digital societies to study emergent social phenomena? Research is needed to model phenomena like opinion polarization and information cascades, and to ethically test the impact of potential policy or platform design interventions in these simulated worlds.
\end{itemize}

\subsection*{Bridging Theory, Practice, and People}
\begin{itemize}
    \item \textbf{Cross-Cultural and Demographic Generalization:} How do we ensure that behavioral models are fair and generalize across diverse cultures, languages, and demographic groups? This requires creating new benchmark datasets that capture global diversity and developing methods to detect and mitigate biases present in training data.
    
    \item \textbf{Theory-Driven and Interdisciplinary Modeling:} How can we more deeply integrate established theories from psychology, sociology, and economics into data-driven models? This involves creating systems that are not only empirically powerful but also scientifically grounded and interpretable, fostering collaborative frameworks that bridge the gap between social science theory and AI practice.
    
    \item \textbf{Real-World Validation and Deployment:} How can we establish effective partnerships to validate and deploy behavioral models in real-world settings while maintaining academic rigor and ethical standards? This includes developing best practices for collaboration between academia, industry, and government to ensure that research translates into responsible and beneficial applications.
\end{itemize}



Finally, as we stand at the cusp of what we identified as the fourth major phase in the study of communication, driven by unprecedented access to digital content and behavioral data, we should remember these sayings:

\textit{We're actually much better at planning the flight path of an interplanetary rocket (rocket science) than we are at managing the economy, merging two corporations, or even predicting how many copies of a book will sell (behavior prediction). So why is it that rocket science \textbf{seems} hard, whereas problems having to do with people - which arguably are much harder - seem like they ought to be \textbf{just} a matter of common sense (easily predictable)?} - Duncan J. Watts       

\begin{center}
    And,
\end{center}

\textit{Nothing in Nature is random (unpredictable). A thing appears random only through the incompleteness of our knowledge (ignorance).} - Baruch Spinoza


Further, as we reflect on the scientific and empirical challenges to the notion of free will, it is instructive to recognize that these questions have been deeply explored in philosophical and religious traditions for millennia. The Bhagavad Gita, a foundational text of Indian philosophy, offers a profound perspective on agency and action. It suggests that what we perceive as free will may, in fact, be shaped by forces beyond our conscious control—nature, past experiences, and even the divine. This resonates with the findings we have made in this thesis, as well as the findings of modern behavioral science and neuroscience, which increasingly point to the limits of conscious agency.



For example, in Chapter 3, Verse 27, the Gita states:

\begin{quote}
    ``Prakṛteḥ kriyamāṇāni guṇaiḥ karmāṇi sarvaśaḥ;
    Ahaṅkāra-vimūḍhātmā kartāham iti manyate.''
\end{quote}

\textit{``All actions are performed by the modes of material nature, but a person deluded by false identification with the ego thinks, `I am the doer.' ''}

And in Chapter 11, Verses 33 and 34, Krishna tells Arjuna:

\begin{quote}
    ``droṇaṁ cha bhīṣhmaṁ cha jayadrathaṁ cha
    karṇaṁ tathānyān api yodha-vīrān 
    mayā hatāṁs tvaṁ jahi mā vyathiṣhṭhā
    yudhyasva jetāsi raṇe sapatnān''
\end{quote}

\textit{``Drona, Bhishma, Jayadratha, Karna, and other great warriors have already been killed by Me. You only be an instrument (nimitta) in the fight.''}

These verses articulate a philosophy of action that acknowledges the limits of personal agency and encourages detachment from the fruits of action. The Gita's perspective is not one of fatalism, but rather an invitation to act with awareness of the larger forces at play—recognizing that while we must act, we are not the sole authors of our actions. This ancient wisdom aligns with contemporary scientific insights, suggesting the nature, causes, and limits of human agency.
