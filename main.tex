%\documentclass[hidelinks,11pt,a4paper]{report}
\documentclass[PhD]{iiitd}
% \usepackage[left=3.0 cm,right=4.0 cm,top=4.0 cm,bottom=5.0 cm]{geometry}
\usepackage{svg} % this has to come above graphicx line
\usepackage[utf8]{inputenc}
\usepackage{lipsum}  
\usepackage{graphicx}
\usepackage{pdflscape}  % For rotating content
\usepackage{graphicx}   % Required for rotating tables
\usepackage{hyperref}
%\usepackage{devanagari}
\usepackage[T1]{fontenc} % this is specially added to handle angel brackets
\usepackage{amsfonts} % for Z symbol
\usepackage{colortbl} % this is for colored columns
\usepackage{multirow} % for merging multiple rows in a table
\usepackage{multicol} % for merging multiple cols in a table
\usepackage{makecell} % for line breaks in table cell
\usepackage{booktabs} % this is for toprule and bottomrule in tables
\usepackage{xcolor} % color only one cell of a table
\usepackage{pdflscape} % one page landscape
%\usepackage[numbers,sort&compress]{natbib}
\usepackage{cite}
\usepackage{amsmath,amsfonts,amssymb}
\let\oldcite\cite
\renewcommand{\cite}[1]{\citep{#1}}
\input{math_commands.tex}
\setcounter{tocdepth}{5}
\setcounter{secnumdepth}{6}
\usepackage{hyperref}

\newcommand{\mathbbm}[1]{\text{\usefont{U}{bbm}{m}{n}#1}}
\input{behavior_model_macros}

\definecolor{gr}{RGB}{ 217, 234, 211 }
\definecolor{bl}{RGB}{ 201, 218, 248 }
\definecolor{rd}{RGB}{ 244, 204, 204 }
\definecolor{or}{RGB}{ 249, 203, 156 }


%%% For the xelatex (and other LaTeX friends) logos
\usepackage{hologo}

%%% For the awesome fontawesome icons!
\usepackage{fontawesome}


%%% For accessing system, OTF and TTF fonts
%%% (would have been loaded by polylossia anyway)
\usepackage{fontspec}
\usepackage{xunicode} %% loading this first to avoid clash with bidi/arabic

%%% For language switching -- like babel, but for xelatex
\usepackage{polyglossia}
\usepackage{placeins}
\setmainlanguage{english}
\setotherlanguages{hindi,sanskrit} %% or other languages
\newfontfamily\devanagarifont[Script=Devanagari]{Noto Serif Devanagari}
\usepackage{tabularx}
\usepackage{enumitem}

% Set global spacing for all enumerate environments
\setlist[enumerate]{topsep=0.5em} % Adjust the value of topset as needed



\begin{document}

% \maketitle
\input{cover}

\pagenumbering{roman}
\setcounter{page}{1}

% \chapter*{Certificate}
% \addcontentsline{toc}{chapter}{Certificate} 
% TEXT TO BE ADDED


\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface} 
\begin{table}[!th]
    \centering
    \begin{tabularx}{1\textwidth}{X}
        
        \textit{We're actually much better at planning the flight path of an interplanetary rocket (rocket science) than we are at managing the economy, merging two corporations, or even predicting how many copies of a book will sell (behavior prediction). So why is it that rocket science \textbf{seems} hard, whereas problems having to do with people - which arguably are much harder - seem like they ought to be \textbf{just} a matter of common sense (easily predictable)?} - Duncan J. Watts\\

        \begin{center}
                Also,
        \end{center}\\


        \textit{If the brain were so simple we could understand (predict) it, we would be so simple we couldn't.} - Emerson Pugh\\
        
        
        
        \begin{center}
            But,
        \end{center}\\
        
        \textit{Nothing in Nature is random (unpredictable). A thing appears random only through the incompleteness of our knowledge (ignorance).} - Baruch Spinoza\\

        \begin{center}
                While,
        \end{center}\\
        

        Ignorance is bliss. - Thomas Gray\\
        
        \begin{center}
                but,
        \end{center}\\

        \textit{Timendi causa est nescire.} (Ignorance is (also) the cause of fear.) - Seneca\\

        \begin{center}
                And,
        \end{center}\\
        
        \textit{What would life be if we had (only fear and) no courage to attempt anything?} - Vincent Van Gogh\\
        
    \end{tabularx}
    
\end{table}


As a computer scientist working on problems of human behavior, I am often asked why I chose this particular domain. The questions come from various perspectives - whether such problems are better suited for psychologists and marketers, why these problems are interesting at all, whether human behavior contains too much randomness to be mathematically tractable, if the problems are ill-defined, and if they are even objectively solvable. For the ones interested in the art of diction, I try to lay out my motivations in the quotes above. But for others, I try to explain through a simple narrative and answer the questions more scientifically in Chapter-1. 



\textit{Behavior is unsolved}. Let me tell you a little story. Kelin is an eager advertiser who releases a campaign on Facebook one Friday evening, paying \$1000 to run an ad across California. With each launch, she silently sends a prayer that the ad resonates, draws clicks, leads to purchases of her product, ensures her campaign's success, and lands her the long-awaited promotion. Come Monday morning, Kelin witnesses human behavior in all its varied glory (within the platform's constraints, obviously): she has received 28 comments on her post, 867 likes, 9045 views, 349 clicks, and 28 purchases. Satisfied but seeking improvement, she tweaks a few words she feels might better appeal to Californians and relaunches. This time, her metrics jump by 10.8\%. Puzzled by the reasons, but pleased with the outcome, she presses on.


From my perspective, Kelin and countless others like her are replicating what the pioneering botanist Gregor Mendel did in the 1800s. The difference is that the subjects for Mendel were peas and for Kelin, it is humans. Mendel was trying to solve the puzzle of why some pea plants are tall and some small, some green while some yellow, and some pea seeds round while some wrinkled. The modern-day Kelins are trying to solve what makes people click, comment, like, and purchase, why certain words perform better in California while others in Texas, how behavior can be modified, and so on. Mendel's laboratory was his 2-acre Moravian monastery farm. Kelin's laboratory is the digital landscape of Facebook, Twitter, YouTube, TikTok, Google, and her website. 

Before Mendel, the general understanding of heredity was one of: \textit{Spontaneous Generation} (organisms could arise spontaneously from non-living matter), \textit{Lamarckism} (traits acquired by an organism during its lifetime could be passed on to its offspring), \textit{Blending Inheritance} (traits of offspring were a blend of the traits of their parents), and \textit{Preformationism} (miniature versions of organisms existed within the reproductive cells of parents). Today, 150 years later, we know to a very high degree of certainty, how traits in organisms arise and their mechanism of inheritance, to the point that we can calculate the probability of a certain type of rare cancer in the offspring of two given parents. However, Kelin's problem of who will click on her ad and how to maximize it remains unsolved and is often considered not worthy enough to be solved by science. Before the heroics of Mendel and Darwin, even the \textit{science} of heredity was considered a domain of philosophy and not worth the \textit{seriousness} of science. 


Rocket science is considered the hardest of sciences. \textit{It is solved.} It is solved to the extent that interplanetary launches over millions of kilometers can be planned to an accuracy of a few meters. Yet human conduct stays inscrutable, quirky, maddingly difficult to forecast and optimize for. It is unsolved to the extent that even opinion polls conducted right before the day of the election give opposite results to what is the actual outcome the next day. In my opinion, if behavior is not the problem to be worth solving, then what is!




%The world is its own best model. - Rodney Brooks

\clearpage

%\chapter*{Abstract}
%\addcontentsline{toc}{chapter}{Abstract}  
%XXX: To be done
%\clearpage


% \chapter{Course Work}
% CSE660A - Trustworthy AI Systems [Credits - 2]
% Final Grade: A-

% CSE641 - Deep Learning [Credits - 4]
% Final Grade: B

% CSE556 - Natural Language Processing [Credits - 4]
% Final Grade: B

% CSE542 - Statistical Machine Learning [Credits - 4]
% Final Grade: A-

% PIS790A - Independent Study [Credits - 2]
% Final Grade: A

% CGPA: 8.63 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter*{Acknowledgments}
\addcontentsline{toc}{chapter}{Acknowledgments}  
\begin{table}[th]
    \centering
    \begin{tabular}{c}
\begin{sanskrit}कार्पण्यदोषोपहतस्वभाव:\end{sanskrit}\\
\begin{sanskrit}पृच्छामि त्वां धर्मसम्मूढचेता: |\end{sanskrit}\\
\begin{sanskrit}यच्छ्रेय: स्यान्निश्चितं ब्रूहि तन्मे\end{sanskrit}\\
\begin{sanskrit}शिष्यस्तेऽहं शाधि मां त्वां प्रपन्नम् ||\end{sanskrit}BG \begin{sanskrit}2:7||\end{sanskrit}\\\\
\begin{sanskrit}मयि सर्वाणि कर्माणि संन्यस्याध्यात्मचेतसा |\end{sanskrit}\\
\begin{sanskrit}निराशीर्निर्ममो भूत्वा युध्यस्व विगतज्वर: ||\end{sanskrit}BG \begin{sanskrit}3:30||\end{sanskrit}\\\\
\begin{sanskrit}
    नैव किञ्चित्करोमीति युक्तो मन्येत तत्त्ववित् |\end{sanskrit}\\
\begin{sanskrit} पश्यञ्शृण्वन्स्पृशञ्जिघ्रन्नश्नन्गच्छन्स्वपञ्श्वसन् ||\end{sanskrit}\\
\begin{sanskrit}प्रलपन्विसृजन्गृह्ण्न्नुन्मिषन्निमिषन्नपि |\end{sanskrit}\\
\begin{sanskrit}इन्द्रियाणीन्द्रियार्थेषु वर्तन्त इति धारयन् ||\end{sanskrit}BG \begin{sanskrit}5:8-9||\end{sanskrit}\\
    
\end{tabular}
\end{table}


This is an attempt to capture and thank those who have shaped my journey. Albeit, due to indirect and latent relations, this list will remain non-exhaustive despite my best attempts, it is still an attempt worth making.

In no particular order (with names of the organizations where I met these giants): Rajiv Ratn Shah (IIIT-D), Changyou Chen (SUNY at Buffalo), Ranjeeta Rani (GMPS), Roger Zimmerman (National University of Singapore), Debanjan Mahata (Bloomberg), Jessy Junyi Li (University of Texas at Austin), Balaji Krishnamurthy (Adobe MDSR), Sridhar Gantimahapatruni (Adobe), Jayakumar Subramanian (Adobe MDSR), Amanda Stent (Bloomberg), Anil Seth (FIITJEE), Dhruva Sahrawat (IIIT-D), Yifang Yin (National University of Singapore), Mika Hama (Second Language Testing Institute), Payman Vafaee (Columbia University), Pankaj Bansal (Adobe), Mohit Srivastava (Adobe), Gaurav Jain (Adobe), Shubham Yadav (NSIT), Rohit Jain (NSIT), Mohd Khwaja Salik (NSIT), Pratham Nawal (NSIT), Mayank Singh (NSIT), Somesh Singh (BITS-Pilani Goa), Aanisha Bhattacharyya (Adobe MDSR), Varun Khurana (IIIT-D), Rita Yadav (GMPS), Prabha Sinha (GMPS), Neeta Pandit (GMPS), Geetha Nair (GMPS), Swami Sarvapriyananda (Ramakrishna Mission), and finally my parents, Sushil Singla and Neena Singla, and my brother, Aman Singla.


Hopefully, I can return whatever I have gathered from these individuals back to society.
\clearpage


\include{abstract}
\clearpage

{\small\tableofcontents}
\pagenumbering{gobble}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction: The Two Cultures of Behavioral Sciences}
\pagenumbering{arabic}
\setcounter{page}{1}


\begin{comment}
    

The motivation of what the problem is and why we need to solve the problem can come from social algorithms, marketing, and also LLMs, human alignment.

The motivation of how we need to solve the problem can come from shannon, LLM training, and two approaches of statistical learning.

<communicator, content/item/message, channel, time, receiver, effect/rating>
<content, effect> prediction and selecting content to optimize effect -> recommendation, CTR prediction
receiver optimization -> personalization

What we are proposing -> better CTR prediction especially in low data settings, effective content generation, better explanation, 



\end{comment}



Behavior as a modality\footnote{A \textit{modality} is defined in terms of information, such that a modality is a medium through which information is conveyed \cite{liang2022foundations,grifoni2009multimodal,martin2001annotation}. Similarly, a multimodal distribution is defined as having more than one peak in the probability distribution describing the nature of information.} occurs in the process of communication. Communication includes all of the procedures by which one mind may affect another \cite{shannon-weaver-1949}. This includes all forms of expression, such as words, gestures, speech, pictures, and musical sounds. Communication can be seen as being composed of seven modalities (Fig.~\ref{fig:factors-of-communication}): (the communicator, message, time of message (or time of receipt), channel, receiver, time of effect, and effect). These modalities may vary independently of each other \cite{khandelwal2023large,khurana-etal-2023-synthesizing,si2023long,khurana2023behavior} and carry signals about each other \cite{khurana-etal-2023-synthesizing,bhattacharya2023video}. The message as a modality carries information from the communicator to receiver and encodes information generated by the communicator. Similarly, behavior (\textit{aka} effect) as a modality carries information from the receiver and encodes information generated by the receiver. This is often a continuous cycle, where behavior generated in the previous cycle becomes the message of the next cycle, thus forming a conversation. 



Different fields of behavioral sciences deal with different parts of behavior. I will give a broad overview of these fields in the upcoming paragraphs, but two streams have emerged broadly in behavioral sciences: explanation and prediction of behavior (receiver effect) \cite{breiman2001statistical,hofman2017prediction,shmueli2010explain}. 


\begin{figure*}[!t]
  \centering
  \includegraphics[width=1.0\textwidth]{images/factors of communication.pdf}
  \caption{Communication process can be defined by seven factors: Communicator, Message, Time of message, Channel, Receiver, Time of effect, and Effect. Any message is created to serve an end goal. For marketers, the end goal is to bring in the desired receiver effect (behavior) (like clicks, purchases, likes, and customer retention). The figure presents the key elements in the communication pipeline - the marketer, message, channel, receivers, and finally, the receiver effect.   \label{fig:factors-of-communication}}
\end{figure*}

Historically, behavioral social scientists have sought explanations of human behavior that can provide interpretable causal mechanisms behind human functioning. A few prominent examples are Milgram's \cite{milgram1978obedience} and Asch's \cite{asch1948doctrine} experiments on persuasion, explaining the causal mechanism of obedience to authority. The approach of theorizing has worked in physical sciences where the data is plentiful, and theories make unambiguous predictions but have not been too successful in \textit{predicting} social outcomes in behavioral sciences \cite{open2015estimating,tetlock2017expert,forecasting2023insights}. In fact, many studies have shown that expert human opinions fare similar to non-experts (\textit{e.g.}, predicting economic and political trends \cite{tetlock2017expert}, societal change: \cite{forecasting2023insights}, and advertising success: \cite{singh2024measuring}), and the opinion of non-expert population is roughly the same as a random coin toss in predicting behavior (\textit{e.g.}, predicting cascades \cite{tan2014effect} or image memorability \cite{isola2013makes}). At the same time, causal mechanisms have their own merits; most notably, they help decision-makers (often humans) to make intuitive sense of the situation and make their next decision based on it. 


In parallel, due to the availability of human behavior data at scale, researchers in machine learning are showing a growing interest in traditionally behavioral science topics, such as messaging strategies leading to persuasion \cite{habernal2016makes,kumar2023persuasion,luu2019measuring,bhattacharya2023video}, information diffusion \cite{cheng2014can,martin2016exploring}, and most importantly, prediction and predictability of human behavior \cite{choi2012predicting,song2010limits}. Machine learning approaches bring with them the culture of (training and) testing their models on large real-world datasets and pushing the state-of-the-art in terms of predictive accuracies; at the same time, often, ML approaches can only be operated as black boxes with no direct mechanism to explain predictions \cite{salganik2019bit,singla2022audio}.



In the prediction community, different subfields have emerged dealing with the different parts of the problem of optimization of human behavior. For instance, advertisement personalization studies how to optimize (choose) \textit{receiver} for a given message \cite{chandra2022personalization}, and recommendation systems study how to \textit{choose content} from a set of pre-decided contents for a given receiver to elicit a certain effect \cite{herlocker2004evaluating}. A popular problem within the prediction community is the effect prediction problems, for example, clickthrough (CTR) prediction \cite{mcmahan2013ad}, Twitter cascade prediction \cite{cheng2014can,martin2016exploring}, sales prediction \cite{choi2012predicting,pryzant2017predicting}, content memorability prediction \cite{isola2011makes,khosla2015understanding,si2023long}, \textit{etc}. There are also works to optimize the time of the message to elicit certain effect \cite{newstead2010cost,si2023long}. Some of the major problems studied in behavioral sciences is given below. We note that all the factors of communication are studied independently in their own light with the aim of achieving the desired effect. 


\begin{enumerate}
\item Problem related to Sender optimization:
    \begin{enumerate}
        \item \textbf{Source Optimization}: \textit{Who} should send a particular message over a channel to a specific audience to get the desired behavior? This includes selecting between different brand voices, influencers, spokespersons, or organizational entities based on authority, trustworthiness, and audience affinity.
    \end{enumerate}
    

\item Problems related to Receiver optimization:
    \begin{enumerate}
        \item \textbf{Personalization}: Identifying the optimal receiver for a specific content-channel-time combination to maximize engagement and conversion probability.
        \item \textbf{Customer Targeting}: 
        Determining which content should be delivered to a target audience segments, targeted based on multi-dimensional attributes (geographic, demographic, behavioral, and psychographic) through particular channels.
        \item \textbf{Customer Segmentation}: Strategic division of the customer base into distinct, actionable groups based on shared characteristics, behaviors, or value propositions to enable differentiated marketing approaches.
        \item \textbf{Social Network Analysis}: Modelling the interconnectedness of receivers (and senders) together in a graph to describe social phenomena like contagion and homophily.
        \item \textbf{Lookalike Modeling}:  Identifying and targeting prospective customers who share similar characteristics, behaviors, and propensities with existing high-value customers or target audiences.
        \item \textbf{Market surveys}: Systematic collection and analysis of primary data about target markets, customer preferences, and competitive landscape to inform marketing decisions.
        \item \textbf{Identity stitching}: Probabilistic and deterministic matching of cross-channel, cross-device customer actions to create unified customer profiles and journey maps.
        \item \textbf{Behavior Explanation}: Discovering causal mechanisms behind a receiver action.
    \end{enumerate}
    
\item Problems related to Content optimization:
    \begin{enumerate}
        \item \textbf{Recommender Systems}: Identifying the optimal content that should be delivered next to a certain receiver given a fixed channel, time and a repository of contents (and their corresponding senders)?
        \item \textbf{A/B Testing}: A randomized experiment involving two or more variants with the goal of discovering which variant of a message performs better with a certain audience.
        \item \textbf{Propensity Modelling or Engagement Modelling}: Modeling probability of engagement in terms of actions like Clickthrough, social media actions such as likes and shares for a certain audience, sender, and campaign.
        \item \textbf{Transsuasion}: Conversion of a content from low-performing to high-performing for a given audience, sender, and time, while maintaining the content's intent, style, and emotional impact.
        \item \textbf{Transcreation}: Conversion of a content designed for one audience (like a particular culture) to another audience, while maintaining the content's intent, style, and emotional impact.
        \item \textbf{Search Engine Optimization}: Improving the quality and quantity of website traffic to a website from search engines by doing content optimization.
        \item \textbf{Performant Content Generation}: Generate content that can perform better for a given audience, sender, time, and goal.
        \item \textbf{Argument Mining}: Automatic extraction and identification of argumentative structures from natural language text.
        \item \textbf{Persuasion Strategies}: Use of rhetorical devices (such as emotion, social identity, and scarcity) to optimize the effect of a message on a certain audience.
    \end{enumerate}

\item Problems related to Channel optimization:
    \begin{enumerate}
        \item \textbf{Channel Optimization}: Optimizing channels for a particular audience, sender, time, and goal.
        \item \textbf{Marketing Mix Modeling}: Measuring and attributing the impact of various decisions like channel investments, discounts, promotional campaigns in their contribution to engagement and sales. 
        \item \textbf{Auction Design} and \textbf{Bidding}: Mechanisms to discover the cost of attention of a certain receiver to a particular sender, time, and campaign goal.
    \end{enumerate}
    
\item Problem related to Time optimization:
    \begin{enumerate}
        \item \textbf{Send Time Optimization}: Determining the optimal timing for message delivery for a certain receiver, sender, content combination.
    \end{enumerate}
        
\end{enumerate}





%Notably, this is the forward path of communication in which, as time progresses, a message originates, travels in a channel, is received by the subscribers, and finally generates an effect. 
Effect (or behavior) over a content can also enable us to understand about the content, the communicator, the receiver, or the time. Therefore, efforts have also been made to extract information about the content itself from the behavior it generates. For instance, using keystroke movements \cite{plank2016keystroke} and eye movements to improve natural language processing \cite{klerke2016improving,khurana-etal-2023-synthesizing}. Similarly, the fields of human alignment and reinforcement learning with human feedback (RLHF) try to use human behavioral signals of likes, upvotes, downloads, and annotations of a response's helpfulness to improve content generation - both text \cite{kreutzer2018can,stiennon2020learning,ziegler2019fine,nakano2021webgpt,si2023long} and images \cite{lee2023aligning,pressman2023simulacra,wu2023better,khurana2023behavior}.




% XXX: to be improved
In social science and computational social science cultures, research is carried out to discover causal effects and model them. 
For instance, propaganda and mass communication studies \cite{mcquail1987mass,krippendorff2018content,lasswell1948structure,lasswell1971propaganda} try to understand the culture, time, authors, recipients in a non-invasive manner using the messages exchanged, and persuasion studies \cite{petty1981effects,chaiken1980heuristic} where the persuasion strategy present in the content is identified and correlated with (un)successful efforts of persuasion. 




A common theme that runs through both research cultures in behavioral sciences is the intent to control behavior. Explanation and prediction are intermediate steps to control and hence optimize behavior. Optimizing behavior means to fulfill the communicator's objectives by controlling the other six parts of the communication process (Fig.~\ref{fig:factors-of-communication}). Due to the problem space being large, the solution needs a general understanding of human behavior as opposed to being domain-specific. 


The characteristic that marks the digital age is the prevalence of human behavioral data in huge repositories. This data is \textit{big} (allowing to model heterogeneity), \textit{always-on} (allowing to look in the past as well as live measurements), observational (as opposed to reactive), but also \textit{incomplete} (does not capture all that is happening everywhere everytime in a single repository) and \textit{algorithmically confounded} (generated as a byproduct of an engineering process with a goal) \cite{salganik2019bit}. While the predictive culture has tried to make use of some of this data in the form of social media datasets like Twitter \cite{tumasjan2010predicting,asur2010predicting} and Instagram \cite{kim2020multimodal}, Google trends \cite{choi2012predicting,carriere2013nowcasting}, Wikipedia \cite{generous2014global,de2021general,mestyan2013early}, shopping websites \cite{krumme2013predictability,de2015unique} and other data sources \cite{brockmann2006scaling,song2010limits,miritello2013limited}, these efforts are limited, in the sense of being dependent on one or a few chosen platforms, able to answer a limited set of questions, and restricted by access to private data. We want a model that can understand (predict and explain) \textit{human behavior in general} as opposed to modeling a particular effect (retweet prediction) on a particular platform (\textit{e.g.} Twitter) for a certain type of users.
This problem carries parallels with the problem being solved in the natural language processing (NLP) community, where supervised models in NLP are limited by the amount of supervision available and being able to answer one question (for which the supervised model was trained). The problem was solved by developing Large Language Models (LLMs), which are general purpose models capable of \textit{understanding language}, and hence can solve natural language tasks like sentiment analysis, question answering, email generation, and language translation in zero-shot (\textit{i.e.} without needing any explicit training for that task) \cite{devlin2018bert,brown2020language,radford2018improving,raffel2020exploring,radford2019language}.





\begin{figure*}[h]
  \centering
  \includegraphics[width=1.0\textwidth]{images/levels of analysis.pdf}
  \caption{Levels of content analysis. The figure lists tasks and their sample outputs arranged in a hierarchy \cite{shannon-weaver-1949}. This is roughly based on levels of language. Notably, humans are good at predicting the first three levels but not the last level \cite{tetlock2017expert,forecasting2023insights,tan2014effect,isola2013makes}. 
  \label{fig:levels of content analysis}
  }
\end{figure*}


Similarly, how do we develop a model capable of understanding behavior \textit{in general}? With the intent to answer this question, we take motivation from LLMs, where the idea is to train a model on a data-rich task. The task chosen to train LLMs is the next-word prediction, and the dataset is the text collected from the entire internet. The next-word prediction task is a data-rich task that can be trained on the huge text repositories from the internet. The intuition is that two approaches have always worked for neural networks: larger model sizes and more data for training \cite{mikolov2013efficient,devlin2018bert,radford2018improving,raffel2020exploring}. Going from a few million tokens of text \cite{mikolov2013efficient,radford2018improving} to a trillion tokens \cite{touvron2023llama,brown2020language} leads to an increase in the transfer learning capability leading to performance improvements over a wide variety of natural language tasks. 


The digital revolution has provided us with huge repositories of data. We leverage the human behavior repositories available on the internet for this general-purpose human behavior model. The format of this data is the general communication model shown in Fig.~\ref{fig:factors-of-communication} consisting of communicator, message, time of message, channel, receiver, time of effect, and effect. Due to the incomplete nature of behavioral repositories, all the factors are usually not always available. However, a subset is always available, and we show that the data scale, along with a large model, helps make a general behavior understanding model \cite{khandelwal2023large}. We call this model, Large Content and Behavior Model (LCBM). We show that LCBM can predict behavior, explain it, and generate a message to bring about certain behavior \cite{si2023long,khandelwal2023large,khurana2023behavior}. 



\textit{Are general LLMs unable to solve behavioral problems?} A question that arises is whether LLMs, which already learn trillions of text tokens, are able to understand and predict behavior. We investigate that question over several large models, including GPT-3.5 \cite{brown2020language}, GPT-4 \cite{openai2023gpt4}, Llama-13 B and LLama-7B \cite{touvron2023llama}, and find that they do not seem to have any behavioral capabilities. The reason for this is that large language models only include one factor (message) out of the 7-factor communication model (Fig.~\ref{fig:factors-of-communication}) while considering other parts as ``noise'' (for instance, see \cite{biderman2022datasheet,penedo2023refinedweb}). This systematic purge of communicator, receiver, channel, time, and, most importantly, behavior causes the models not to develop any behavioral capabilities (Level-C of Shannon and Weaver \cite{shannon-weaver-1949}). As an example, Llava \cite{liu2023visual}, a recent large language and vision model (VLM) trained by connecting a vision encoder with a language model, shows that after training on a few hundred thousand instructions, the language model can now ``see'', and is able to answer questions on the images. However, the questions all lie in the first two levels of content analysis shown in Fig.~\ref{fig:levels of content analysis}. The reason is that the instructions used to align the image encoder with the downstream LLM all lie in the first two levels while ignoring the last two. 
In the upcoming chapters, we explore how we can train a general behavior model and how including the other factors of communication back in training data helps in understanding human behavior.


\textit{Outline for the upcoming chapters}: Following the two traditions of behavioral sciences, in this work, in Chapter-\ref{chatper:Explaining Behavior: Persuasion Strategies}, I start with a more traditional approach to behavior explanation, where I cover the first works on extracting persuasion strategies in advertisements (both images and videos) \cite{kumar2023persuasion,bhattacharya2023video}. The contributions of these works include constructing the largest set of generic persuasion strategies based on theoretical and empirical studies in marketing, social psychology, and machine learning literature and releasing the first datasets to enable the study and model development for the same. These works have been deployed to understand the correlation between the kinds of marketing campaigns and customer behavior measured by clicks, views, and other marketing key performance indicators (KPIs). 

Following this, in Chapter-\ref{chatper:Content and Behavior Models}, I delve into the question of modeling behavior. The key insight behind this chapter is that behavior is always produced by a receiver in response to a content sent by a sender at a time. We model behavior together with the pieces of sender, receiver, time, and content. We show that while large language models already model content, they do not model the other pieces of sender, receiver, and time. We model these factors together and show emergent abilities in understanding behavior. 


Next, in Chapter~\ref{chapter:Encoding Behavior To Improve Content Understanding}, we analyze the communication process in more detail. As behavior is the signal a receiver emits when a sender sends a content; similarly, one can see this behavior emitted as a content in the next cycle, where the receiver becomes the sender, and the sender becomes the receiver. Therefore, we ask the question if we can understand the content better by modeling behavior. For example, a person's heightened state of emotional response, like dilated pupils and sweat, while watching an action scene from the movie Jurassic Park gives us much information about the scene itself. Today's models are built only on content (the Jurassic Park movie itself) while ignoring the human behavioral responses over the content. Behavioral responses like likes, shares, comments, replay graphs, and upvotes are freely available and waiting to be integrated into the workflow to understand content better. We show evidence for this hypothesis by improving LLMs over a host of different tasks across all four modalities of language, audio, text, and video. 

%the more modern approach of behavior prediction and leveraging the huge repositories of behavior data available. First, we propose models to integrate behavior with relatively smaller language models like BERT \cite{devlin2018bert}, and show that the resultant models can understand content better than the base models \cite{khurana-etal-2023-synthesizing}. Then, we propose an approach to integrate behavior and content together as part of a single model. We call these models Large Content and Behavior Models (LCBM) \cite{khandelwal2023large}. We show that these models can predict and explain behavior. 

%Diagram for chapters - XXX

Communication, arguably, happens to reach a shared goal between the sender and the receiver \cite{smith2003animal}. We humans are exceptional in our capacity to cooperate with strangers. Language allowed our ancestors to cooperate and helped to resolve conflicts by exchanging information, though this includes invented fictions, social constructions, and other imagined realities \cite{misyak2016instantaneous,mccroskey2015introduction,smith1997major}. We start developing the ability to communicate and persuade as small children \cite{perner1985john}. Further, this capability is not unique to our species but is observed in other animals as well, in both conspecific \cite{hare2000chimpanzees,smith2003animal} and interspecific \cite{krebs1984animal,fouts200235} scenarios. For example, adult birds across various genera perform ``broken wing displays'' - deceptive behaviors where they feign injury to appear vulnerable. By tricking nearby predators into viewing them as easy prey, the birds can successfully lure threats away from their chicks \cite{griffin2001animal}. Therefore, in the final chapter of my thesis (Chapter-\ref{chatper:Generating Content Leading to Optimal Behavior}), we show that by modeling the entire communication workflow together, we can generate messages to elicit certain behavior, resulting in behavior optimization. We show this both for the domain of text, by taking the illustrative case of memorability and generating content that is more memorable in long-term \cite{si2023long}, and images, by generating images that are more performant, \textit{i.e.}, can result in more likes \cite{khurana2023behavior}. 


I cover the following works in this thesis:
\begin{enumerate}
    \item Persuasion Strategies in Advertisements, AAAI, 2023, (covered in Chapter-\ref{chatper:Explaining Behavior: Persuasion Strategies})
    \item A Video Is Worth 4096 Tokens: Verbalize Videos To Understand Them In Zero Shot, EMNLP, 2023, \textbf{Nominated for best paper award} (covered in Chapter-\ref{chatper:Explaining Behavior: Persuasion Strategies})
    \item Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior, ICLR, 2024, \textbf{Nominated for best paper award} (covered in Chapter-\ref{chatper:Content and Behavior Models})
    \item Synthesizing Human Gaze Feedback for Improved NLP Performance, EACL, 2023 (covered in Chapter-\ref{chapter:Encoding Behavior To Improve Content Understanding})
    \item Teaching Human Behavior Improves Content Understanding Abilities Of VLMs, Arxiv preprint (under review), 2024 (covered in Chapter-\ref{chapter:Encoding Behavior To Improve Content Understanding})
    \item Long-Term Ad Memorability: Understanding and Generating Memorable Ads, WACV, 2025 (covered in Chapter-\ref{chatper:Generating Content Leading to Optimal Behavior})
    \item Measuring And Improving Engagement of Text-to-Image Generation Models, Arxiv preprint (under review), 2024 (covered in Chapter-\ref{chatper:Generating Content Leading to Optimal Behavior})
\end{enumerate}



\begin{comment}
    They mentioned that the broad problem of communication can be studied at three levels: technical, semantic, and effectiveness. 
    \textbf{Level A: Technical.} How accurately can the symbols of communication be transmitted?
    
    \textbf{Level B: Semantic.} How precisely do the transmitted symbols convey the desired meaning?
    
    \textbf{Level C: Effectiveness.} How well does the received meaning induce the desired conduct in the receiver?
    
    These three levels build on top of each other. Thus, solving the problem at Level C necessarily requires solving the corresponding problems at Levels A and B.
    
    Since the publication of this seminal paper, the tremendous growth in the field of telecommunications, particularly the advent of the Internet and mobile devices, has led to affordable, wide-scale solutions for Level A.
    With the recent advances in large language models (LLMs) such as BERT \citep{devlin2018bert}, GPT-3 and 4 \citep{brown2020language,openai2023gpt4}, T5 \citep{raffel2020exploring}, and many more, we have witnessed a significant improvement in the performance of various Natural Language Processing (NLP) tasks. LLMs in zero- or few-shot settings can easily handle tasks such as question answering, summarization, translation, and many more. This has helped us progress towards solving Level B to a large extent. However, there has been limited progress in Level C, the effectiveness problem. 
    
    
    
    Effectiveness refers to communicating to fulfill the communicator's objectives.
\end{comment}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\include{chapter-explaining-behavior}
\include{chapter-content-behavior-model}
\include{chapter-encoding-behavior-to-improve-content-understanding}
\include{chapter-generating-content-optimize-behavior}
\include{Conclusion}
\include{publications}






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\listoftables
\addcontentsline{toc}{chapter}{LIST OF TABLES}
\listoffigures
\addcontentsline{toc}{chapter}{LIST OF FIGURES}
%\listofsymbols
%\addcontentsline{toc}{chapter}{LIST OF SYMBOLS}
\addcontentsline{toc}{chapter}{Bibliography}
%\bibliographystyle{plainnat}
\begin{singlespace}
\bibliography{ref}
\end{singlespace}

\end{document}
